# -*- coding: utf-8 -*-
"""“Swin-Transformer.ipynb”的副本

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UrA2j9-JT6UnpWvTQKK6qi2g_0KAVFFJ

**DataSet**
"""
import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
from PIL import Image
import torch
from torch.utils.data import Dataset
from scipy.io import loadmat

class MyDataSet(Dataset):

  def __init__(self, images_path:list, images_class: list, transform=None):
    self.images_path = images_path
    self.images_class = images_class
    self.transform = transform

  def __len__(self):
    return len(self.images_path)

  def __getitem__(self, item):
    img = Image.open(self.images_path[item]).convert('gray')
    # if img.mode != 'RGB':
    #   raise ValueError("image: {} isn't a RGB mode.".format(self.images_path[item]))
    label = self.images_class[item]

    if self.transform is not None:
      img = self.transform(img)

    return torch.tensor(img), torch.tensor(label)

"""MRI DataSet-cc359"""
class DatasetMri(Dataset):
    def __init__(self, root, mask, transform=None):

        file_list = [os.path.join(root, file_name) for file_name in os.listdir(root)]
        self.dataset = []
        for each in file_list:
            try:
                data = np.load(each)
            except ValueError as e:
                print(each)

            print(data.shape)
            self.dataset.extend(data)
            print(len(self.dataset))
        # if train:
        #     self.data = self.data[:2000]
        # else:
        #     self.data = self.data[2000:]
        self.transform = transform
        self.mask = mask

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, index):
        assert index <= len(self.dataset), f"index must lower than {len(self.dataset)}"
        data = self.dataset[index]
        data = data[:, :, 0] + 1j * data[:, :, 1] #cc-dataset:complex data

        data = np.fft.ifftshift(data, axes=(-2, -1))
        data = ifft2c(data, norm='ortho')
        data = np.fft.fftshift(data, axes=(-2, -1))

        data = (data - np.min(np.abs(data))) / (np.max(np.abs(data)) - np.min(np.abs(data)) + 1e-5)

        u_img, u_k, label_k = undersample(data, self.mask, True)
        size = data.shape
        size = (2, size[0], size[1])
        under_img = torch.zeros(size)
        under_k = torch.zeros(size)

        full_img = torch.zeros(size)
        full_k = torch.zeros(size)

        under_img[0] = torch.from_numpy(u_img.real)
        under_img[1] = torch.from_numpy(u_img.imag)

        under_k[0] = torch.from_numpy(u_k.real)
        under_k[1] = torch.from_numpy(u_k.imag)

        full_img[0] = torch.from_numpy(data.real)
        full_img[1] = torch.from_numpy(data.imag)

        full_k[0] = torch.from_numpy(label_k.real)
        full_k[1] = torch.from_numpy(label_k.imag)

        if self.transform:
            return self.transform(data), self.transform(self.ifft(self.fft(data, True), False))
        else:
            return under_img, under_k, full_img, full_k
        # data = (data - np.min(data)) / (np.max(data)  - np.min(data))


    def ifft(self, x: "numpy input", mask: "use mask or not"):
        x = np.fft.ifftshift(x)
        if mask:
            x = self.mask * x
        x = np.abs(np.fft.ifft2(x))
        x = (x - np.min(x)) / (np.max(x) - np.min(x) + 1e-5)
        return x

    def fft(self, x: "numpy input"):
        x = np.fft.fft2(x)
        return np.fft.fftshift(x)

def fft2c(x, norm="ortho"):
    '''
    Centered fft
    Note: fft2 applies fft to last 2 axes by default
    :param x: 2D onwards. e.g: if its 3d, x.shape = (n,row,col). 4d:x.shape = (n,slice,row,col)
    :return:
    '''
    # axes = (len(x.shape)-2, len(x.shape)-1)  # get last 2 axes
    axes = (-2, -1)  # get last 2 axes
    res = np.fft.fftshift(np.fft.fft2(np.fft.ifftshift(x, axes=axes), norm=norm), axes=axes)
    return res


def ifft2c(x, norm="ortho"):
    '''
    Centered ifft
    Note: fft2 applies fft to last 2 axes by default
    :param x: 2D onwards. e.g: if its 3d, x.shape = (n,row,col). 4d:x.shape = (n,slice,row,col)
    :return:
    '''
    axes = (-2, -1)  # get last 2 axes
    res = np.fft.fftshift(
        np.fft.ifft2(np.fft.ifftshift(x, axes=axes), norm=norm), axes=axes)
    return res

def undersample(x, mask, centred=False, norm='ortho', noise=0):
    '''
    Undersample x. FFT2 will be applied to the last 2 axis
    Parameters
    ----------
    x: array_like
        data
    mask: array_like
        undersampling mask in fourier domain
    norm: 'ortho' or None
        if 'ortho', performs unitary transform, otherwise normal dft
    noise_power: float
        simulates acquisition noise, complex AWG noise.
        must be percentage of the peak signal
    Returns
    -------
    xu: array_like
        undersampled image in image domain. Note that it is complex valued
    x_fu: array_like
        undersampled data in k-space
    '''
    assert x.shape == mask.shape
    # zero mean complex Gaussian noise
    noise_power = noise
    nz = np.sqrt(.5) * (
            np.random.normal(0, 1, x.shape) + 1j * np.random.normal(0, 1, x.shape))
    nz = nz * np.sqrt(noise_power)

    if norm == 'ortho':
        # multiplicative factor
        nz = nz * np.sqrt(np.prod(mask.shape[-2:]))
    else:
        nz = nz * np.prod(mask.shape[-2:])

    if centred:
        x_f = fft2c(x, norm=norm)
        x_fu = mask * (x_f + nz)
        x_u = ifft2c(x_fu, norm=norm)
        return x_u, x_fu, x_f
    else:
        x_f = np.fft.fft2(x, norm=norm)
        x_fu = mask * (x_f + nz)
        x_u = np.fft.ifft2(x_fu, norm=norm)
        return x_u, x_fu, x_f

class IXI_dataset(Dataset):
    def __init__(self, root, mask, train, transform=None):
        self.data = IXI_data(root, train)
        self.transform = transform
        self.mask = mask

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        assert index <= len(self.data), f"index must lower than {len(self.data)}"
        data = self.data[index]
        data = (data - np.min(data)) / (np.max(data) - np.min(data) + 1e-8)
        data = np.array(data, dtype=np.complex128)
        # data = (data - np.min(np.abs(data))) / (np.max(np.abs(data)) - np.min(np.abs(data)) + 1e-5)
        u_img, u_k, label_k = undersample(data, self.mask, True)
        u_img.imag = 0

        size = data.shape
        size = (2, size[0], size[1])
        under_img = torch.zeros(size)
        under_k = torch.zeros(size)

        full_img = torch.zeros(size)
        full_k = torch.zeros(size)

        under_img[0] = torch.from_numpy(u_img.real)
        under_img[1] = torch.from_numpy(u_img.imag)

        under_k[0] = torch.from_numpy(u_k.real)
        under_k[1] = torch.from_numpy(u_k.imag)

        full_img[0] = torch.from_numpy(data.real)
        full_img[1] = torch.from_numpy(data.imag)

        full_k[0] = torch.from_numpy(label_k.real)
        full_k[1] = torch.from_numpy(label_k.imag)

        if self.transform:
            return self.transform(data), self.transform(self.ifft(self.fft(data, True), False))
        else:
            return under_img, under_k, full_img, full_k

    def ifft(self, x: "numpy input", mask: "use mask or not"):
        x = np.fft.ifftshift(x)
        if mask:
            x = self.mask * x
        x = np.abs(np.fft.ifft2(x))
        x = (x - np.min(x)) / (np.max(x) - np.min(x) + 1e-5)
        return x

    def fft(self, x: "numpy input", mask):
        x = np.fft.fft2(x)
        x = np.fft.fftshift(x)
        if mask:
            x = self.mask * x
        return x

def IXI_ifft(x: "numpy input", mask: "use mask or not"):
    temp = np.zeros([x.shape[0], 1, x.shape[2], x.shape[3]])
    temp = x[:, 0, :, :] + 1j*x[:, 1, :, :]
    x = np.fft.ifftshift(temp)
    if mask:
        x = mask * x
    x = np.abs(np.fft.ifft2(x))
    x = np.resize(x, (x.shape[0], 1, x.shape[1], x.shape[2]))
    x = (x - np.min(x)) / (np.max(x) - np.min(x) + 1e-5)
    return x

def IXI_fft(x: "numpy input", mask):
    x = np.fft.fft2(x)
    x = np.fft.fftshift(x)
    temp = np.zeros([x.shape[0], 2*x.shape[1], x.shape[2], x.shape[3]])
    # print(x.shape)
    # print(temp.shape)
    # print(x.real.shape)
    # print(temp[:, 0, :, :].shape)
    temp[:, 0, :, :] = np.resize(x.real, (batch_size, 256, 256))
    temp[:, 1, :, :] = np.resize(x.imag, (batch_size, 256, 256))
    x = temp
    if mask:
        x = mask * x
    return x
"""**Utils**"""

import os
import sys
import json
import pickle
import random

import torch
from tqdm import tqdm

import matplotlib.pyplot as plt




def write_pickle(list_info: list, file_name: str):
    with open(file_name, 'wb') as f:
        pickle.dump(list_info, f)


def read_pickle(file_name: str) -> list:
    with open(file_name, 'rb') as f:
        info_list = pickle.load(f)
        return info_list

"""**Drop_path**"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.checkpoint as checkpoint
import numpy as np
from typing import Optional

def drop_path_f(x, drop_prob: float=0., training: bool=False):
  if drop_prob == 0. or not training:
    return x
  keep_prob = 1 - drop_prob
  shape = (x.shape[0], ) + (1, ) * (x.ndim - 1)
  random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
  random_tensor.floor_()
  output = x.div(keep_prob) * random_tensor
  return output

class DropPath(nn.Module):
  def __init__(self, drop_prob=None):
    super(DropPath, self).__init__()
    self.drop_prob = drop_prob

  def forward(self, x):
    return drop_path_f(x, self.drop_prob, self.training)

"""**Path_Embedding,Window_partition,Path_reverse,PathMerging, MLP**"""

def window_partition(x, window_size: int):

  B, H, W, C = x.shape
  x = x.view(B, H//window_size, window_size, W//window_size, window_size, C)
  windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
  return windows

def window_reverse(windows, window_size:int, H: int, W: int):

  B = int(windows.shape[0] / (H * W / window_size / window_size))
  x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
  x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
  return x

class PatchEmbed(nn.Module):

  def __init__(self, patch_size=4, in_c=3, embed_dim=96, norm_layer=None):
    super().__init__()
    patch_size = (patch_size, patch_size)
    self.patch_size = patch_size
    self.in_chans = in_c
    self.embed_dim = embed_dim
    self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)
    self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()

  def forward(self,x):
    _, _, H, W = x.shape
    
    pad_input = (H % self.patch_size[0] != 0) or (W % self.patch_size[1] != 0)
    if pad_input:
      x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1], 0, self.patch_size[0] - H % self.patch_size[0], 0, 0))

    x = self.proj(x)
    _, _, H, W = x.shape
    x = x.flatten(2).transpose(1, 2)
    x = self.norm(x)
    return x, H, W

class PatchMerging(nn.Module):
  def __init__(self, dim, norm_layer=nn.LayerNorm):
    super().__init__()
    self.dim = dim
    self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
    self.norm = norm_layer(4*dim)

  def forward(self, x, H, W):
    B, L, C = x.shape
    assert L == H * W, "input feature has wrong size"
    x = x.view(B, H, W, C)
    pad_input = (H % 2 == 1) or (W % 2 == 1)
    x = F.pad(x, (0, 0, 0, W%2, 0, H%2))
    x0 = x[:, 0::2, 0::2, :]
    x1 = x[:, 1::2, 0::2, :]
    x2 = x[:, 0::2, 1::2, :]
    x3 = x[:, 1::2, 1::2, :]
    x = torch.cat([x0, x1, x2, x3], dim=-1)
    x = x.view(B, -1, 4*C)
    x = self.norm(x)
    x = self.reduction(x)

    return x

class Mlp(nn.Module):
  def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
    super().__init__()
    out_features = out_features or in_features
    hidden_features = hidden_features or in_features

    self.fc1 = nn.Linear(in_features, hidden_features)
    self.act = act_layer()
    self.drop1 = nn.Dropout(drop)
    self.fc2 = nn.Linear(hidden_features, out_features)
    self.drop2 = nn.Dropout(drop)

  def forward(self, x):
    x = self.fc1(x)
    x = self.act(x)
    x = self.drop1(x)
    x = self.fc2(x)
    x = self.drop2(x)

    return x

"""**WindowAttention(shifted) and Swin_Transformer block**"""

from torch.nn.modules import padding
class WindowAttention(nn.Module):

  def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0, proj_drop=0.):
    super().__init__()
    self.dim = dim
    self.window_size = window_size
    self.num_heads = num_heads
    head_dim = dim // num_heads
    self.scale = head_dim ** -0.5

    self.relative_position_bias_table = nn.Parameter(
        torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)
    )
    coords_h = torch.arange(self.window_size[0])
    coords_w = torch.arange(self.window_size[1])
    coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing='ij'))
    coords_flatten = torch.flatten(coords, 1)
    relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
    relative_coords = relative_coords.permute(1, 2, 0).contiguous()
    relative_coords[:, :, 0] += self.window_size[0] - 1
    relative_coords[:, :, 1] += self.window_size[1] - 1
    relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
    relative_position_index = relative_coords.sum(-1)
    self.register_buffer("relative_position_index", relative_position_index)

    self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias)
    self.attn_drop = nn.Dropout(attn_drop)
    self.proj = nn.Linear(dim, dim)
    self.proj_drop = nn.Dropout(proj_drop)
    nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)
    self.softmax = nn.Softmax(dim=1)

  def forward(self, x, mask: Optional[torch.Tensor] = None):

    B_, N, C = x.shape
    qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C//self.num_heads).permute(2, 0, 3, 1, 4)
    q, k, v = qkv.unbind(0)
    q = q * self.scale
    attn = (q @ k.transpose(-2, -1))
    relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1],
                                                        self.window_size[0] * self.window_size[1], -1)
    relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()
    attn = attn + relative_position_bias.unsqueeze(0)

    if mask is not None:
      nW = mask.shape[0]
   
      attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
      attn = attn.view(-1, self.num_heads, N, N)
      attn = self.softmax(attn)

    else:
      attn = self.softmax(attn)
    
    attn = self.attn_drop(attn)
    x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
    x = self.proj(x)
    x = self.proj_drop(x)
    return x

class SwinTransformerBlock(nn.Module):
  def __init__(self, dim, num_heads, window_size=7, shift_size=0, mlp_ratio=4, qkv_bias=True, drop=0, attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
    super().__init__()
    self.dim = dim
    self.num_heads = num_heads
    self.window_size = window_size
    self.shift_size = shift_size
    self.mlp_ratio = mlp_ratio
    assert 0 <= self.shift_size < self.window_size, "shift_size must in 0-window_size"

    self.norm1 = norm_layer(dim)
    self.attn = WindowAttention(
        dim, window_size=(self.window_size, self.window_size), num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop
    )
    self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
    self.norm2 = norm_layer(dim)
    mlp_hidden_dim = int(dim * mlp_ratio)
    self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

  def forward(self, x, attn_mask):
    H, W =self.H, self.W
    # H, W = 64, 64
    B, L, C = x.shape
    # print(f"B: {B}, L :{L}, H: {H}, W: {W}, C: {C}")
    assert L == H * W, "input feature has wrong size"

    shortcut = x
    x = self.norm1(x)
    x = x.view(B, H, W, C)
    pad_l = pad_t = 0
    pad_r = (self.window_size - W % self.window_size) % self.window_size
    pad_b = (self.window_size - H % self.window_size) % self.window_size
    x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))
    _, Hp, Wp, _ = x.shape

    if self.shift_size > 0:
      shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
    else:
      shifted_x = x
      attn_mask = None

    x_windows = window_partition(shifted_x, self.window_size)
    # print(f"after partition: {x_windows.shape}")
    x_windows = x_windows.view(-1, self.window_size * self.window_size, C)
    # print(f"after view: {x_windows.shape}")

    attn_windows = self.attn(x_windows, mask=attn_mask)
    shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)

    if self.shift_size > 0:
      x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
    else:
      x = shifted_x

    if pad_r > 0 or pad_b > 0:
      x = x[:, :H, :W, :].contiguous()

    x = x.view(B, H*W, C)

    x = shortcut + self.drop_path(x)
    x = x + self.drop_path(self.mlp(self.norm2(x)))

    return x

class BasicLayer(nn.Module):
  def __init__(self, dim, depth, num_heads, window_size, mlp_ratio=4, qkv_bias=True, drop=0, attn_drop=0., 
        drop_path=0, norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):
    super().__init__()
    self.dim = dim
    self.window_size = window_size
    self.use_checkpoint = use_checkpoint
    self.shift_size = window_size // 2

    self.blocks = nn.ModuleList([
          SwinTransformerBlock(
              dim=dim, num_heads=num_heads, window_size=window_size,shift_size=0 if (i % 2 == 0) else self.shift_size,
              mlp_ratio=mlp_ratio,
              qkv_bias=qkv_bias,
              drop=drop,
              attn_drop=attn_drop,
              drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
              norm_layer=norm_layer) for i in range(depth)
    ])

    if downsample is not None:
      self.downsample = downsample(dim=dim, norm_layer=norm_layer)
    else:
      self.downsample = None

  def create_mask(self, x, H, W):
    Hp = int(np.ceil(H / self.window_size)) * self.window_size
    Wp = int(np.ceil(W / self.window_size)) * self.window_size

    img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)
    h_slices = (slice(0, -self.window_size),
                slice(-self.window_size, -self.shift_size),
                slice(-self.shift_size, None))
    w_slices = (slice(0, -self.window_size),
                slice(-self.window_size, -self.shift_size),
                slice(-self.shift_size, None))
    
    cnt = 0
    for h in h_slices:
      for w in w_slices:
        img_mask[:, h, w, :] = cnt
        cnt += 1
    mask_windows = window_partition(img_mask, self.window_size)
    mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
    # print(f"attn_mask.size: {attn_mask.shape}")

    return attn_mask

  def forward(self, x, H, W):
    attn_mask = self.create_mask(x, H, W)
    for blk in self.blocks:
      blk.H, blk.W = H, W
      if not torch.jit.is_scripting() and self.use_checkpoint:
        x = checkpoint.checkpoint(blk, x, attn_mask)
      else:
        x = blk(x, attn_mask)
    if self.downsample is not None:
      x = self.downsample(x, H, W)
      H, W = (H + 1) // 2, (W + 1) // 2

    return x, H, W

class SwinTransformer(nn.Module):
  def __init__(self, patch_size=4, in_chans=1, num_classes=1000,
          embed_dim=96, depths=(2, 2, 3, 2), num_heads=(3, 6, 6, 6),
          window_size=7, mlp_ratio=4, qkv_bias=True, 
          drop_rate=0, attn_drop_rate=0, drop_path_rate=0.1,
          norm_layer=nn.LayerNorm, patch_norm=True,
          use_checkpoint=False, **kwargs):
    super().__init__()

    self.num_classes = num_classes
    self.num_layers = len(depths)
    self.embed_dim = embed_dim
    self.patch_norm = patch_norm
    self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))
    self.mlp_ratio = mlp_ratio

    self.patch_embed = PatchEmbed(
        patch_size=patch_size, in_c=in_chans, embed_dim=embed_dim,
        norm_layer=norm_layer if self.patch_norm else None
    )
    self.pos_drop=nn.Dropout(p=drop_rate)

    dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]

    self.layers = nn.ModuleList()
    for i_layer in range(self.num_layers):
      layers = BasicLayer(dim=int(embed_dim * 2 ** i_layer),
                  depth=depths[i_layer],
                  num_heads = num_heads[i_layer],
                  window_size=window_size,
                  mlp_ratio=self.mlp_ratio,
                  qkv_bias=qkv_bias,
                  drop=drop_rate,
                  attn_drop=attn_drop_rate,
                  drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
                  norm_layer=norm_layer,
                  downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,
                  use_checkpoint=use_checkpoint)
      self.layers.append(layers)
    self.norm = norm_layer(self.num_features)
    self.avgpool = nn.AdaptiveAvgPool1d(1)
    self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
    self.apply = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()

    self.apply(self._init_weights)

  def _init_weights(self, m):
    if isinstance(m, nn.Linear):
      nn.init.trunc_normal_(m.weight, std=.02)
      if isinstance(m, nn.Linear) and m.bias is not None:
        nn.init.constant_(m.bias, 0)
    elif isinstance(m, nn.LayerNorm):
      nn.init.constant_(m.bias, 0)
      nn.init.constant_(m.weight, 1.0)

  def forward(self, x):
    x, H, W = self.patch_embed(x)
    x = self.pos_drop(x)

    for layer in self.layers:
      x, H, W = layer(x, H, W)
    
    x = self.norm(x)
    x = self.avgpool(x.transpose(1, 2))
    x = torch.flatten(x, 1)
    x = self.head(x)
    return x

"""**Train**"""

import os
import argparse

import torch
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
# from torchvision import transforms


"""**Unet**"""
"""------------------------------------------------------------------------"""

import torch
import torch.nn

class SwinMRI(nn.Module):
  def __init__(self, patch_size=4, in_chans=1, num_classes=1000,
          embed_dim=96, depths=(2, 2, 6, 2), num_heads=(3, 6, 12, 24),
          window_size=7, mlp_ratio=4, qkv_bias=True, 
          drop_rate=0, attn_drop_rate=0, drop_path_rate=0.1,
          norm_layer=nn.LayerNorm, patch_norm=True,
          use_checkpoint=False, **kwargs):
    super().__init__()

    self.num_classes = num_classes
    self.num_layers = len(depths)
    self.embed_dim = embed_dim
    self.patch_norm = patch_norm
    self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))
    self.mlp_ratio = mlp_ratio

    self.patch_embed = PatchEmbed(
        patch_size=patch_size, in_c=in_chans, embed_dim=embed_dim,
        norm_layer=norm_layer if self.patch_norm else None
    )
    self.pos_drop=nn.Dropout(p=drop_rate)

    dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]

    self.layers = nn.ModuleList()
    for i_layer in range(self.num_layers):
      layers = BasicLayer(dim=int(embed_dim * 2 ** i_layer),
                  depth=depths[i_layer],
                  num_heads = num_heads[i_layer],
                  window_size=window_size,
                  mlp_ratio=self.mlp_ratio,
                  qkv_bias=qkv_bias,
                  drop=drop_rate,
                  attn_drop=attn_drop_rate,
                  drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
                  norm_layer=norm_layer,
                  downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,
                  use_checkpoint=use_checkpoint)
      self.layers.append(layers)
    self.norm = norm_layer(self.num_features)
    self.avgpool = nn.AdaptiveAvgPool1d(1)
    self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
    self.apply = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()

    self.apply(self._init_weights)

  def _init_weights(self, m):
    if isinstance(m, nn.Linear):
      nn.init.trunc_normal_(m.weight, std=.02)
      if isinstance(m, nn.Linear) and m.bias is not None:
        nn.init.constant_(m.bias, 0)
    elif isinstance(m, nn.LayerNorm):
      nn.init.constant_(m.bias, 0)
      nn.init.constant_(m.weight, 1.0)

  def forward(self, x):
    x, H, W = self.patch_embed(x)
    x = self.pos_drop(x)

    stack = []
    stack.append([x, H, W])
    for layer in self.layers:
      x, H, W = layer(x, H, W)
      stack.append([x, H, W])
    # x = self.norm(x)
    # x = self.avgpool(x.transpose(1, 2))
    # x = torch.flatten(x, 1)
    # x = self.head(x)
    return x, stack


def UnEmbedding(x, h, w):
  x = x.transpose(1, 2).contiguous().view(parser['batch_size'], -1, h, w)
  return x


class FeatureUnit(nn.Module):
  def __init__(self, is_imageSpace = True):
    super().__init__()
    if is_imageSpace:
        self.conv1 = nn.Conv2d(1, 1, 3, 1, 1)
        self.downSwin = SwinMRI(in_chans=1)
        # print("现在在图像域！")
    else:
        self.conv1 = nn.Conv2d(2, 2, 3, 1, 1)
        self.downSwin = SwinMRI(in_chans=2)
        # print("现在在频率域！")
    self.unsample = nn.PixelShuffle(2)


  def forward(self, x):
    # print(x.shape)
    x = self.conv1(x)
    x, stack = self.downSwin(x)
    # x = self.UnEmbedding(x)
    # x = self.unsample(x)


    return x, stack

class up(nn.Module):
  def __init__(self, in_chans, norm_shape, upsample=False):
    super().__init__()
    self.conv1 = nn.Conv2d(in_channels=in_chans, out_channels=in_chans, kernel_size=3, stride=1, padding=1)
    self.relu = nn.LeakyReLU()
    self.conv2 = nn.Conv2d(in_channels=in_chans, out_channels=in_chans, kernel_size=3, stride=1, padding=1)
    # self.norm = nn.BatchNorm2d(in_chans)
    self.norm = nn.LayerNorm(norm_shape)
    self.UpSample = nn.PixelShuffle(2)
    self.upsample = upsample

    self.dropout = nn.Dropout(0.3)
    # self.dropout2 = nn.Dropout(0.2)


  def forward(self, *input):
    x0 = input[0]
    x1 = input[1]
    if self.upsample:
      x0 = self.UpSample(x0)
    x = self.conv1(torch.cat((x0, x1), dim=1))
    x = self.norm(x)
    x = self.relu(x)
    x = self.conv2(x)
    x = self.norm(x)
    x = self.relu(x)                      #先BN后RELU
    x = self.dropout(x)

    return x


class ImageRestoration(nn.Module):

  def __init__(self, in_chans, is_image_space = True):
    super().__init__()
    self.up1 = up(in_chans, norm_shape=(8, 8))
    self.up2 = up(in_chans//2, norm_shape=(16, 16), upsample=True)
    self.up3 = up(in_chans//4, norm_shape=(32, 32), upsample=True)
    self.up4 = up(in_chans//8, norm_shape=(64, 64), upsample=True)
    self.upsample = nn.PixelShuffle(2)
    #
    self.bottleneck = nn.Conv2d(in_chans//2, in_chans//2, 1, 1)
    self.bottleConv = nn.Conv2d(in_chans//2, in_chans//2, 3, 1, 1)
    self.bottleNorm = nn.LayerNorm((8, 8))
    self.bottleRelu = nn.LeakyReLU()
    if is_image_space:
        self.conv_last = nn.Conv2d(6, 1, 3, 1, 1)
    else:
        self.conv_last = nn.Conv2d(12, 2, 3, 1, 1)

  def forward(self, x, stack):
    x1, h, w = stack.pop()
    x0 = UnEmbedding(x, h, w)
    x1 = UnEmbedding(x1, h, w)

    x = self.bottleneck(x0)
    x = self.bottleConv(x)
    x = self.bottleNorm(x)
    x = self.bottleRelu(x)
    x = self.bottleneck(x)

    # print('\n', x0.shape, x1.shape)
    x = self.up1(x0, x1)

    x1, h, w = stack.pop()
    x1 = UnEmbedding(x1, h, w)
    # print(x.shape, x1.shape)
    x = self.up2(x, x1)

    x1, h, w = stack.pop()
    x1 = UnEmbedding(x1, h, w)
    # print(x.shape, x1.shape)
    x = self.up3(x, x1)

    x1, h, w = stack.pop()
    x1 = UnEmbedding(x1, h, w)
    # print(x1.size())
    x = self.up4(x, x1)

    for i in range(2):
      x = self.upsample(x)
      # print(x.shape)

    x = self.conv_last(x)
    # print(x)

    return x


class Swin_Unet(nn.Module):
  def __init__(self, in_chans, is_imageSpace):
    super().__init__()
    # 特征提取部分，表示整个U形网络的左半部分
    self.feature = FeatureUnit(is_imageSpace=is_imageSpace)
    # 上采样部分，表示整个U形网络的右半部分
    self.image_restore = ImageRestoration(in_chans, is_imageSpace)

  
  def forward(self, x):
    x, stack = self.feature(x)
    x = self.image_restore(x, stack[:-1])
    return x

def  fft(input):
    # (N, 2, W, H) -> (N, W, H, 2)
    # print(type(input))
    # input = input.permute(0, 2, 3, 1)
    input_complex = input[:, 0, :, :] + 1j*input[:, 1, :, :]
    input_complex = input_complex.view(batch_size, 1, 256, 256)
    input_complex = torch.fft.fft2(input_complex, norm="ortho")
    out = torch.cat([input_complex.real, input_complex.imag], dim = 1)

    # (N, W, H, 2) -> (N, 2, W, H)
    # input = input.permute(0, 3, 1, 2)
    return out

def ifft(input):
    input_complex = input[:, 0, :, :] + 1j * input[:, 1, :, :]
    input_complex = input_complex.view(batch_size, 1, 256, 256)
    input_complex = torch.fft.ifft2(input_complex, norm="ortho")

    out = torch.cat([input_complex.real, input_complex.imag], dim=1)

    return out


class cross_domain(nn.Module):
    def __init__(self, mask):
        super().__init__()

        self.mask = torch.from_numpy(mask)
        self.model_img = Swin_Unet(1536, is_imageSpace=False)
        self.model_fre = Swin_Unet(1536, is_imageSpace=False)
        self.model_img2 = Swin_Unet(1536, is_imageSpace=False)


    def forward(self, *input):
        under_img = input[0]
        under_freq = input[1]

        x_img = self.model_img(under_img)
        # 转化到频率域(是否会存在CPU-CUDA的训练问题)
        x_img_to_freq = fft(x_img)
        x_freq = self.model_fre(x_img_to_freq)
        #转化到空间域
        x_img = ifft(x_freq)
        x_img = self.model_img2(x_img)


        return x_img



def fft_map(x: 'tensor'):
    x = torch.fft.fftn(x)
    x_real = x.real
    x_imag = x.imag

    return x_real, x_imag

def G_loss(grd_img, recon, loss_fn):
    alpha = 1
    beta = 0.0005
    gamma = 0.2
    grd_real, grd_vis = fft_map(grd_img)
    rec_real, recon_vis = fft_map(recon)
    # 空域损失
    loss_img = loss_fn(grd_img, recon)
    # 频域损失
    loss_freq = (loss_fn(grd_real, rec_real) + loss_fn(grd_vis, recon_vis)) / 2
    # 生成器对抗损失
    # loss_adv_G = loss_disc(recon, True)
    print(f"loss_img: {loss_img:.3f}, loss_freq: {loss_freq * beta:.3f}")

    return alpha * loss_img + beta * loss_freq

def D_loss(Disc, grd_img, recon, loss_disc):

    D_real = Disc(grd_img)
    D_fake = Disc(recon.detach())
    loss_real = loss_disc(D_real, True)
    loss_fake = loss_disc(D_fake, False)
    loss = (loss_real + loss_fake) * 0.5
    return loss


import os
import argparse

import torch
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from skimage.metrics import structural_similarity, peak_signal_noise_ratio
import cv2 as cv
from torchvision import transforms

# from model import swin_tiny_patch4_window7_224 as create_model
# from utils import read_split_data, train_one_epoch, evaluate
def calculate_ssim(img1, img2):
    img1 = img1.data.cpu().float().squeeze().numpy()
    img2 = img2.data.cpu().float().squeeze().numpy()
    if not img1.shape == img2.shape:
        raise ValueError("Input images must have same dimensions")
    assert img1.shape[0] == img2.shape[0], "img1 and img2 must have same number of image"
    img1 = img1.astype(np.float64)
    img2 = img2.astype(np.float64)
    ssims = []
    for i in range(img1.shape[0]):
        ssims.append(structural_similarity(img1[i][0], img2[i][0]))
    return np.sum(ssims)/img1.shape[0]

def calculate_psnr(img1, img2):
    img1 = img1.data.cpu().float().squeeze().numpy()
    img2 = img2.data.cpu().float().squeeze().numpy()
    if not img1.shape == img2.shape:
        raise ValueError("Input images must have same dimensions")
    assert img1.shape[0] == img2.shape[0], "img1 and img2 must have same number of image"
    img1 = img1.astype(np.float64)
    img2 = img2.astype(np.float64)
    psnr = []
    for i in range(img1.shape[0]):
        psnr.append(peak_signal_noise_ratio(img1[i], img2[i]))
    return np.sum(psnr) / img1.shape[0]

def tensor2uint(img):
    img = img.data.squeeze().float().clamp_(0, 1).cpu().numpy()
    return np.uint8(img * 255.0).round()

def draw(img_unsample: 'nd array', img_recon, img_grd):
    assert img_unsample.ndim == 2, "we need a 2 dim image"
    plt.figure()
    plt.subplot(131)
    plt.title("unsample")
    plt.imshow(img_unsample, cmap='gray')

    plt.subplot(132)
    plt.title("recon")
    plt.imshow(img_recon, cmap='gray')

    plt.subplot(133)
    plt.title("ground truth")
    plt.imshow(img_grd, cmap='gray')

    plt.show()

def img_save(img: 'tensor', img_path, epoch, type):
    img = img.data.squeeze().float().clamp_(0, 1).cpu().numpy()
    assert img.ndim == 3, "CHW should be right"
    for i in range(img.shape[0]):
          img_buffer = img[i]
          img_buffer = np.uint((img_buffer * 255.0).round())
          path = os.path.join(img_path, '{}_{}_{}.png'.format(type, epoch, i))
          cv.imwrite(path, img_buffer)

def train_one_epoch_copy(model, optimizer, data_loader, test_loader, device, epoch):
    model.train()
    loss_function = torch.nn.L1Loss()
    # loss_function = torch.nn.MSELoss()
    optimizer.zero_grad()
    # optimizer_D.zero_grad()

    data_loader = tqdm(data_loader, file=sys.stdout, colour='green')

    current_step = 0
    for step, data in enumerate(data_loader):
        current_step += 1
        usp_img, usp_freq, full_img, full_k = data
        usp_img = usp_img.to(device)
        usp_freq = usp_freq.to(device)
        full_img = full_img.to(device)
        full_k = full_k.to(device)
        # grd_img, usp_img = grd_img.unsqueeze(1).to(torch.float32), usp_img.unsqueeze(1).to(torch.float32)
        pred = model(*(usp_img, usp_freq, full_img, full_k))

        # loss = loss_function(pred, grd_img.to(device))
        # 针对判别器的损失与步进
        # loss_disc = D_loss(disc, grd_img, pred, Gan_loss)
        # loss_disc.backward()
        # optimizer_D.step()
        # vgg = vgg.to(device)
        loss = G_loss(full_img.to(device), pred, loss_function)
        loss.backward()
        optimizer.step()

        psnr = calculate_psnr(full_img, pred)
        ssim = calculate_ssim(full_img, pred)
        data_loader.desc = "[train epoch {}] loss: {:.3f} psnr: {:.3f}\ssim: {:.3f}".format(epoch,
                                              loss.detach().item(), psnr, ssim)

        if not torch.isfinite(loss):
            print('WARNING: non-finite loss, ending training ', loss)
            sys.exit(1)
        
        # if (epoch+1) % 2 == 0:
        if current_step % parser['checkpoint_test'] == 0:
            model.eval()
            with torch.no_grad():
                avg_ssim = 0.0
                test_loader = tqdm(test_loader, file=sys.stdout, colour='red')
                for idx, test_data in enumerate(test_loader):
                    usp_img, usp_freq, full_img, full_k = test_data
                    usp_img = usp_img.to(device)
                    usp_freq = usp_freq.to(device)
                    full_img = full_img.to(device)
                    full_k = full_k.to(device)

                    pred = model(*(usp_img, usp_freq, full_img, full_k))
                    loss = loss_function(pred, full_img)

                    psnr = calculate_psnr(full_img, pred)
                    ssim = calculate_ssim(full_img, pred)

                    psnr_usp = calculate_psnr(full_img, usp_img)
                    ssim_usp = calculate_ssim(full_img, usp_img)
                    if idx % 30 == 0:
                        # usp_img = tensor2uint(usp_img)
                        # pred = tensor2uint(pred)
                        # grd_img = tensor2uint(full_img)
                        # draw(usp_img[0][0], pred[0][0], grd_img[0][0])
                        plt.imsave(f"./images/usp-cartesian{idx}-{psnr_usp:.3f}-{ssim_usp:.3f}.png", usp_img[0][0].detach().cpu().numpy(), cmap='gray')
                        plt.imsave(f"./images/recon-cartesian{idx}-{psnr:.3f}-{ssim:.3f}.png", pred[0][0].detach().cpu().numpy(), cmap='gray')
                        plt.imsave(f"./images/full-cartesian{idx}.png", full_img[0][0].detach().cpu().numpy(), cmap='gray')
                        print(f"now:{idx}!!!_psnr: {psnr:.3f}, ssim: {ssim:.3f}")
                    test_loader.desc = "[test epoch {}] loss: {:.3f} psnr: {:.3f}\ssim: {:.3f}".format(epoch,
                                                            loss.detach().item(), psnr, ssim)


        optimizer.zero_grad()
        # optimizer_D.zero_grad()



    return loss.detach().item()

def main(args):
    device = torch.device(args['device'] if torch.cuda.is_available() else "cpu")

    if os.path.exists("./weights") is False:
        os.makedirs("./weights")

    tb_writer = SummaryWriter()

    # train_images_path, train_images_label, val_images_path, val_images_label = read_split_data(args['data_path'])

    img_size = 256
    data_transform = {
        "train": transforms.Compose([
                                     transforms.ToTensor(),
                                     transforms.ToPILImage(),
                                     transforms.RandomHorizontalFlip(p=0.2),
                                     transforms.RandomRotation(degrees=20),
                                     transforms.ColorJitter(brightness=0.5),
                                     transforms.ToTensor(),
                      ])}
    #     "val": transforms.Compose([transforms.Resize(int(img_size)),
    #                                transforms.CenterCrop(img_size),
    #                                transforms.ToTensor(),
    #                   ])}

    batch_size = args['batch_size']
    train_dataset = IXI_dataset(root=args['data_path'], train=True,
                               mask=radial_mask, transform=None)
    # train_dataset = DatasetMri(root=r"../datasets/CC-359/Train/",
    #                            mask=radial_mask, transform=None)

    train_loader = torch.utils.data.DataLoader(train_dataset,
                          batch_size=batch_size,
                          shuffle=True,
                          pin_memory=True,
                          num_workers=0,
                          drop_last=True,
                          )
    test_dataset = IXI_dataset(root=args['data_path'], train=False, mask=radial_mask, transform=None)
    # test_dataset = DatasetMri(root=r"../datasets/CC-359/Val/", mask=radial_mask, transform=None)
    test_loader = torch.utils.data.DataLoader(test_dataset,
                          batch_size=batch_size,
                          shuffle=False,
                          pin_memory=True,
                          num_workers=0,
                          drop_last=True,
                          )

    # 设定损失函数与参数
    # Gan_loss = GANLoss()
    # 定义VGG模型     
    # vgg_loss = VGG_loss(True, nn.L1Loss())
    # 设定模型的参数
    # Disc = NLayerDiscriminator(1).to(device)
    # 网络的主要部分，重建主网络
    model = cross_domain(radial_mask).to(device)


    if args['weights'] != "":
        assert os.path.exists(args['weights']), "weights file: '{}' not exist.".format(args['weights'])
        weights_dict = torch.load(args['weights'], map_location=device)
        # 删除有关分类类别的权重
        for k in list(weights_dict.keys()):
            if "head" in k:
                del weights_dict[k]
        print(model.load_state_dict(weights_dict, strict=False))

    if args['freeze_layers']:
        for name, para in model.named_parameters():
            # 除head外，其他权重全部冻结
            if "head" not in name:
                para.requires_grad_(False)
            else:
                print("training {}".format(name))

    pg = [p for p in model.parameters() if p.requires_grad]
    optimizer = optim.AdamW(pg, lr=args['lr'], weight_decay=5E-2)
    # 判别器的模型初始化。
    # pd = [p for p in Disc.parameters() if p.requires_grad]
    # optimizer_D = optim.AdamW(pd, lr=args['lr'], weight_decay=5E-2)


    for epoch in range(args['epochs']):
        # train
        train_loss =  train_one_epoch_copy(model=model,
                        optimizer=optimizer,
                        data_loader=train_loader,
                        test_loader=test_loader,
                        device=device,
                        epoch=epoch)


        tags = ["train_loss", "train_acc", "val_loss", "val_acc", "learning_rate"]
        tb_writer.add_scalar(tags[0], train_loss, epoch)
        # tb_writer.add_scalar(tags[1], train_acc, epoch)
        # tb_writer.add_scalar(tags[2], val_loss, epoch)
        # tb_writer.add_scalar(tags[3], val_acc, epoch)
        tb_writer.add_scalar(tags[4], optimizer.param_groups[0]["lr"], epoch)
        if epoch % 3 == 0:
            torch.save(model.state_dict(), "./weights/IXI-model-cartesian(10).pth")

"""# 新段落"""
def processing_data(path:"raw k-space data"):
    # import nibabel as nb
    list_dir = os.listdir(path)
    dataset = []
    for root in list_dir:
        data = np.load(os.path.join(path, root))
        dataset.extend(data)
    dataset = np.array(dataset)
    return dataset

# 处理原始的IXI数据
def IXI_data(path, train):
    import nibabel as nib
    file_list = [os.path.join(path, file_name) for file_name in os.listdir(path)]
    # data = np.array(nib.load(file_list[10]).get_fdata(), dtype='float32')

    dataset = []
    index = np.arange(0, len(file_list), 10)
    # random.shuffle(index)
    #进行等间隔的验证集选取
    index_valid = index[::5]
    index_train = [x for x in index if x not in index_valid]
    file_list_copy = []
    if train:
        for i in index_train:
            file_list_copy.append(file_list[i])
    else:
        for i in index_valid:
            file_list_copy.append(file_list[i])
    for i in file_list_copy:
        data = np.array(nib.load(i).get_fdata(), dtype='float32')
        print(f"loding :{i}")
        for j in range(data.shape[1]):
            temp = data[:, j, :]
            temp = cv.resize(temp, dsize=(150, 256))
            black = np.zeros((256, 256))
            black[:, 53:203] = temp
            dataset.append(black)

    dataset = np.array(dataset)
    return dataset


if __name__ == '__main__':


    # 数据集原始位置
    batch_size = 24
    data_path = r"E:/MRI/data/IXI/IXI"
    # 掩码矩阵存储位置
    radial_mask_path = r"../mask/brain/cartesian/cartesian_256_256_10_.mat"
    radial_mask = loadmat(radial_mask_path)['Umask']  # 20%
    radial_mask = cv.resize(radial_mask, dsize=(256, 256))
    parser = {'num_classes': 5,
              'epochs': 200,
              'batch_size': batch_size,
              'lr': 0.0001,
              'data_path': r"../datasets/IXI/IXI-T1/",
              'weights': r"./weights/IXI-model-cartesian(10).pth",
              'freeze_layers': False,
              'device': 'cuda:1',
              'checkpoint_test': 300,
              'img_dir': "./images",
              }


    main(parser)

"""
服务器运行依赖：
scikit-image, opencv-python, tqdm
"""
